{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521377, 8)\n",
      "(1521377, 7)\n",
      "   latitude  longitude       cog       sog       rot   heading  anchored  \\\n",
      "0   7.50361   77.58340  0.855833  0.167319 -0.047619  0.877778         0   \n",
      "1   7.57302   77.49505  0.854444  0.169276  0.039683  0.869444         0   \n",
      "2   7.65043   77.39404  0.852222  0.165362  0.039683  0.866667         0   \n",
      "3   7.71275   77.31394  0.855278  0.165362  0.047619  0.869444         0   \n",
      "4   7.77191   77.23585  0.852778  0.159491  0.055556  0.869444         0   \n",
      "\n",
      "   time_diff  longitude_target  \n",
      "0     1393.0          77.49505  \n",
      "1     1583.0          77.39404  \n",
      "2     1285.0          77.31394  \n",
      "3     1259.0          77.23585  \n",
      "4      901.0          77.18147  \n",
      "   latitude  longitude       cog       sog       rot   heading  anchored  \\\n",
      "0   7.50361   77.58340  0.855833  0.167319 -0.047619  0.877778         0   \n",
      "1   7.57302   77.49505  0.854444  0.169276  0.039683  0.869444         0   \n",
      "2   7.65043   77.39404  0.852222  0.165362  0.039683  0.866667         0   \n",
      "3   7.71275   77.31394  0.855278  0.165362  0.047619  0.869444         0   \n",
      "4   7.77191   77.23585  0.852778  0.159491  0.055556  0.869444         0   \n",
      "\n",
      "   time_diff  latitude_target  \n",
      "0     1393.0          7.57302  \n",
      "1     1583.0          7.65043  \n",
      "2     1285.0          7.71275  \n",
      "3     1259.0          7.77191  \n",
      "4      901.0          7.81285  \n"
     ]
    }
   ],
   "source": [
    "# Load your features and labels datasets\n",
    "features = pd.read_csv('data/features.csv')\n",
    "labels = pd.read_csv('data/labels.csv')\n",
    "\n",
    "# Check the structure of both dataframes to ensure they match\n",
    "print(features.shape)  # Ensure the number of rows match in features and labels\n",
    "print(labels.shape)\n",
    "\n",
    "joined_data = features.join(labels, rsuffix=\"_target\") \n",
    "# print(joined_data)\n",
    "\n",
    "joined_data_long = joined_data.drop([\"anchored_target\", \"heading_target\", \"rot_target\", \"sog_target\", \"cog_target\", \"latitude_target\"], axis=1)\n",
    "joined_data_lat = joined_data.drop([\"anchored_target\", \"heading_target\", \"rot_target\", \"sog_target\", \"cog_target\", \"longitude_target\"], axis=1)\n",
    "\n",
    "print(joined_data_long.head())\n",
    "print(joined_data_lat.head())\n",
    "\n",
    "joined_data_long.to_csv(\"data/joined_data_long.csv\", index=False)\n",
    "joined_data_lat.to_csv(\"data/joined_data_lat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target labels (y)\n",
    "X_train_long = joined_data_long.drop(columns=[\"longitude_target\"])\n",
    "y_train_long = joined_data_long[\"longitude_target\"]\n",
    "\n",
    "X_train_lat = joined_data_lat.drop(columns=[\"latitude_target\"])\n",
    "y_train_lat = joined_data_lat[\"latitude_target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude Model Predictions: [80.22893  80.22893  80.22893  ... 21.002035 21.053448 21.165998]\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model for longitude prediction\n",
    "model_long = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, objective='reg:squarederror')\n",
    "model_long.fit(X_train_long, y_train_long)\n",
    "\n",
    "# Optionally, evaluate the model\n",
    "preds = model_long.predict(X_train_long)  # Use test data when available\n",
    "print(f\"Longitude Model Predictions: {preds_long}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude Model Predictions: [ 7.7351913  7.7351913  7.7455487 ... 59.699562  59.699562  59.81051  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train XGBoost model for latitude prediction\n",
    "model_lat = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, objective='reg:squarederror')\n",
    "model_lat.fit(X_train_lat, y_train_lat)\n",
    "\n",
    "# Optionally, evaluate the model\n",
    "preds_lat = model_lat.predict(X_train_lat)  # Use test data when available\n",
    "print(f\"Latitude Model Predictions: {preds_lat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'anchored', 'time_diff'] ['ID', 'time_step']\nexpected anchored, latitude, sog, time_diff, rot, cog, heading, longitude in input data\ntraining data did not have the following fields: time_step, ID",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Now the data is ready for predictions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_test_long \u001b[38;5;241m=\u001b[39m test_data_cleaned\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling_factor\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Drop scaling_factor if not needed for longitude\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m preds_test_long \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_long\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_long\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude Predictions on Test Data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds_test_long\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Similarly for latitude predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/shipping/lib/python3.11/site-packages/xgboost/sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/shipping/lib/python3.11/site-packages/xgboost/core.py:2510\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2508\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[1;32m   2509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[0;32m-> 2510\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[1;32m   2512\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/shipping/lib/python3.11/site-packages/xgboost/core.py:3075\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[1;32m   3070\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[1;32m   3073\u001b[0m     )\n\u001b[0;32m-> 3075\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'anchored', 'time_diff'] ['ID', 'time_step']\nexpected anchored, latitude, sog, time_diff, rot, cog, heading, longitude in input data\ntraining data did not have the following fields: time_step, ID"
     ]
    }
   ],
   "source": [
    "# Load your test dataset\n",
    "test_data = pd.read_csv('task/ais_test.csv')\n",
    "\n",
    "# Drop the 'vesselId' column since it's not needed for the predictions\n",
    "test_data_cleaned = test_data.drop(columns=['vesselId'])\n",
    "\n",
    "# Convert 'time' column to a timestamp\n",
    "test_data_cleaned['time'] = pd.to_datetime(test_data_cleaned['time'])\n",
    "\n",
    "# Convert the 'time' column to a numerical timestep (e.g., seconds since the start of the dataset)\n",
    "# Assuming you want to count the number of seconds since the first timestamp\n",
    "test_data_cleaned['time_step'] = (test_data_cleaned['time'] - test_data_cleaned['time'].min()).dt.total_seconds()\n",
    "\n",
    "# Drop the original 'time' column (if you only want to use 'time_step')\n",
    "test_data_cleaned = test_data_cleaned.drop(columns=['time'])\n",
    "\n",
    "# Now the data is ready for predictions\n",
    "X_test_long = test_data_cleaned.drop(columns=['scaling_factor'])  # Drop scaling_factor if not needed for longitude\n",
    "preds_test_long = model_long.predict(X_test_long)\n",
    "print(f\"Longitude Predictions on Test Data: {preds_test_long}\")\n",
    "\n",
    "# Similarly for latitude predictions\n",
    "X_test_lat = test_data_cleaned.drop(columns=['scaling_factor'])  # Drop scaling_factor if not needed for latitude\n",
    "preds_test_lat = model_lat.predict(X_test_lat)\n",
    "print(f\"Latitude Predictions on Test Data: {preds_test_lat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
